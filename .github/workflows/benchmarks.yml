# =============================================================================
# Benchmarks: Compression and Decompression Performance
# =============================================================================
# Architecture:
#   Stage 1: Build all tools ONCE
#   Stage 2: Prepare benchmark data (download silesia, generate software/logs)
#   Stage 3: Compression benchmarks (data_type Ã— level Ã— threads)
#   Stage 4: Decompression benchmarks (archive_type Ã— threads)
#   Stage 5: Guards and summary
#
# Benchmark data:
#   - silesia-large.gz (~170MB compressed, ~500MB uncompressed) - mixed binary/text
#   - software.archive.gz - synthetic source code patterns
#   - logs.txt.gz - synthetic repetitive log data
# =============================================================================
name: Benchmarks

on:
  push:
    branches: ["*"]
  pull_request:
    branches: ["*"]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  # ============================================================================
  # STAGE 1: Build all tools once
  # ============================================================================
  build:
    name: Build Tools
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake nasm zlib1g-dev

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Build all tools
        run: |
          # Build gzippy
          cargo build --release

          # Build pigz
          make -C pigz

          # Build igzip (static)
          cd isa-l
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF
          make -j$(nproc) igzip || echo "igzip build failed"
          cd ../..

          # Build rapidgzip
          cd rapidgzip/librapidarchive
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DWITH_ISAL=OFF
          make -j$(nproc) rapidgzip || echo "rapidgzip build failed"
          cd ../../..

          # Build zopfli
          make -C zopfli zopfli || echo "zopfli build failed"

      - name: Package binaries
        run: |
          mkdir -p binaries
          cp target/release/gzippy binaries/
          cp pigz/pigz pigz/unpigz binaries/
          cp isa-l/build/igzip binaries/ 2>/dev/null || echo "igzip not available"
          cp rapidgzip/librapidarchive/build/src/tools/rapidgzip binaries/ 2>/dev/null || echo "rapidgzip not available"
          cp zopfli/zopfli binaries/ 2>/dev/null || echo "zopfli not available"
          ls -la binaries/

      - uses: actions/upload-artifact@v4
        with:
          name: binaries
          path: binaries/
          retention-days: 1

  # ============================================================================
  # STAGE 2: Prepare benchmark data
  # Uses `make bench-data` which downloads silesia and generates software/logs
  # Compressed versions are cached in git; uncompressed regenerated on demand
  # ============================================================================
  prepare-data:
    name: Prepare Benchmark Data
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: actions/checkout@v4

      - name: Prepare benchmark data
        run: |
          make bench-data
          ls -lh benchmark_data/

      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-data
          path: benchmark_data/
          retention-days: 1

  # ============================================================================
  # STAGE 3: Compression benchmarks (data_type Ã— level Ã— threads)
  # ============================================================================
  compression-benchmark:
    name: Compress ${{ matrix.data_type }} L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: prepare-data
    strategy:
      fail-fast: false
      matrix:
        data_type: [silesia, software, logs]
        level: [1, 6, 9]
        threads: ["1", "max"]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: benchmark-data
          path: ./benchmark_data

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

          # Map data_type to actual file
          case "${{ matrix.data_type }}" in
            silesia)
              echo "DATA_FILE=benchmark_data/silesia.tar" >> $GITHUB_ENV
              ;;
            software)
              echo "DATA_FILE=benchmark_data/software.archive" >> $GITHUB_ENV
              ;;
            logs)
              echo "DATA_FILE=benchmark_data/logs.txt" >> $GITHUB_ENV
              ;;
          esac

      - name: Verify data exists
        run: |
          ls -lh benchmark_data/
          if [ ! -f "$DATA_FILE" ]; then
            echo "ERROR: $DATA_FILE not found"
            exit 1
          fi
          echo "Using: $DATA_FILE ($(du -h $DATA_FILE | cut -f1))"

      - name: Run compression benchmark
        run: |
          python3 scripts/benchmark_compression.py \
            --binaries ./bin \
            --data-file "$DATA_FILE" \
            --level ${{ matrix.level }} \
            --threads $THREADS \
            --content-type ${{ matrix.data_type }} \
            --output results/compression.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-compress-${{ matrix.data_type }}-l${{ matrix.level }}-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 4: Decompression benchmarks (archive_type Ã— threads)
  # ============================================================================
  decompression-benchmark:
    name: Decompress ${{ matrix.archive_type }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: prepare-data
    strategy:
      fail-fast: false
      matrix:
        archive_type: [silesia, software, logs]
        threads: ["1", "max"]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: benchmark-data
          path: ./benchmark_data

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

          # Map archive_type to actual files
          case "${{ matrix.archive_type }}" in
            silesia)
              # Use the large silesia file for parallel benchmarks, fall back to regular
              if [ -f benchmark_data/silesia-large.gz ]; then
                echo "COMPRESSED_FILE=benchmark_data/silesia-large.gz" >> $GITHUB_ENV
                echo "ORIGINAL_FILE=benchmark_data/silesia-large.bin" >> $GITHUB_ENV
              else
                echo "COMPRESSED_FILE=benchmark_data/silesia-gzip.tar.gz" >> $GITHUB_ENV
                echo "ORIGINAL_FILE=benchmark_data/silesia.tar" >> $GITHUB_ENV
              fi
              ;;
            software)
              echo "COMPRESSED_FILE=benchmark_data/software.archive.gz" >> $GITHUB_ENV
              echo "ORIGINAL_FILE=benchmark_data/software.archive" >> $GITHUB_ENV
              ;;
            logs)
              echo "COMPRESSED_FILE=benchmark_data/logs.txt.gz" >> $GITHUB_ENV
              echo "ORIGINAL_FILE=benchmark_data/logs.txt" >> $GITHUB_ENV
              ;;
          esac

      - name: Verify data exists
        run: |
          ls -lh benchmark_data/
          if [ ! -f "$COMPRESSED_FILE" ]; then
            echo "ERROR: $COMPRESSED_FILE not found"
            exit 1
          fi
          if [ ! -f "$ORIGINAL_FILE" ]; then
            echo "ERROR: $ORIGINAL_FILE not found"
            exit 1
          fi
          echo "Compressed: $COMPRESSED_FILE ($(du -h $COMPRESSED_FILE | cut -f1))"
          echo "Original: $ORIGINAL_FILE ($(du -h $ORIGINAL_FILE | cut -f1))"

      - name: Run decompression benchmark
        run: |
          python3 scripts/benchmark_decompression.py \
            --binaries ./bin \
            --compressed-file "$COMPRESSED_FILE" \
            --original-file "$ORIGINAL_FILE" \
            --threads $THREADS \
            --archive-type ${{ matrix.archive_type }} \
            --output results/decompression.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-decompress-${{ matrix.archive_type }}-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 5: Random data correctness check
  # ============================================================================
  random-data:
    name: Random Data L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        level: [1, 9]
        threads: ["1", "max"]

    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup
        run: |
          chmod +x ./bin/*

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Test random data
        run: |
          # Generate 10MB random data
          dd if=/dev/urandom of=/tmp/random.bin bs=1M count=10 2>/dev/null

          # Compress
          cat /tmp/random.bin | ./bin/gzippy -${{ matrix.level }} -p$THREADS > /tmp/random.gz

          # Decompress with gzippy
          ./bin/gzippy -d < /tmp/random.gz > /tmp/random.out
          diff /tmp/random.bin /tmp/random.out
          echo "âœ“ gzippy roundtrip OK"

          # Verify gzip compatibility
          gzip -d < /tmp/random.gz > /tmp/random.gzip.out
          diff /tmp/random.bin /tmp/random.gzip.out
          echo "âœ“ gzip can decompress gzippy output"

  # ============================================================================
  # STAGE 6: Performance guards and summary
  # ============================================================================
  guards:
    name: Performance Guards
    runs-on: ubuntu-latest
    needs: [compression-benchmark, decompression-benchmark, random-data]
    if: always() && needs.compression-benchmark.result == 'success' && needs.decompression-benchmark.result == 'success'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: results/
          merge-multiple: true

      - name: Collect system info
        run: python3 scripts/collect_system_info.py --output results/system.json

      - name: Aggregate results
        run: |
          python3 scripts/aggregate_benchmark_results.py \
            --input-dir results \
            --output-dir aggregated

      - name: Check performance guards
        id: guards
        run: |
          python3 scripts/check_guards.py \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output guards-report.json

      - name: Generate summary
        run: |
          python3 scripts/generate_summary.py \
            --system results/system.json \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output summary.md

          cat summary.md >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: |
            aggregated/
            results/system.json
            guards-report.json
            summary.md
          retention-days: 30

      - name: Post to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let summary;
            try {
              summary = fs.readFileSync('summary.md', 'utf8');
            } catch {
              summary = '## gzippy Performance Summary\n\nNo results available.';
            }

            const body = summary + '\n\n---\n*Updated: ' + new Date().toISOString() + '*';

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const marker = '# ðŸš€ gzippy Performance Summary';
            const existing = comments.find(c =>
              c.user.type === 'Bot' && c.body && c.body.includes(marker)
            );

            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
