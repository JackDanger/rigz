# =============================================================================
# Decompression Benchmarks: Compare gzippy vs competitors
# =============================================================================
# - Benchmarks decompression of files from gzippy, pigz, gzip, rapidgzip
# - Uses adaptive benchmarking (runs until CV < 5% or max 30 trials)
# - Reports speed in MB/s for each tool at each thread count
# =============================================================================
name: Decompression Benchmarks

on:
  push:
    branches: ["*"]
  pull_request:
    branches: ["*"]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always
  # Don't use -C target-cpu=native in CI - causes SIGILL on heterogeneous runners
  # gzippy uses libdeflate (statically linked) for high performance

jobs:
  # ============================================================================
  # STAGE 1: Build all tools once
  # ============================================================================
  build:
    name: Build Decompression Tools
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake nasm zlib1g-dev

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Build gzippy
        run: |
          cargo build --release
          # Verify binary is self-contained (no custom dynamic libs)
          ldd target/release/gzippy 2>/dev/null || otool -L target/release/gzippy || true

      - name: Build pigz
        run: make -C pigz

      - name: Build rapidgzip
        continue-on-error: true
        run: |
          cd rapidgzip/librapidarchive
          mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DWITH_ISAL=OFF
          make -j$(nproc) rapidgzip || true
          if [ -f tools/rapidgzip ]; then
            cp tools/rapidgzip /tmp/rapidgzip-cli
            echo "âœ“ rapidgzip CLI built"
          fi

      - name: Package binaries
        run: |
          mkdir -p /tmp/binaries
          cp target/release/gzippy /tmp/binaries/
          cp pigz/pigz /tmp/binaries/
          cp pigz/unpigz /tmp/binaries/
          cp /tmp/rapidgzip-cli /tmp/binaries/ 2>/dev/null || echo "rapidgzip not available"
          ls -la /tmp/binaries/

      - uses: actions/upload-artifact@v4
        with:
          name: binaries
          path: /tmp/binaries/
          retention-days: 1

  # ============================================================================
  # STAGE 2: Generate test data and all compressed variants
  # ============================================================================
  prepare-data:
    name: Prepare Test Data
    runs-on: ubuntu-latest
    needs: build
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup binaries
        run: chmod +x ./bin/*

      - name: Generate 100MB text data
        run: |
          python3 -c "
          import random
          random.seed(42)
          words = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', 'and', 'cat']
          with open('/tmp/test-100mb.txt', 'w') as f:
              target = 100 * 1024 * 1024
              written = 0
              while written < target:
                  line = ' '.join(random.choices(words, k=10)) + '\n'
                  f.write(line)
                  written += len(line)
          "
          ls -lh /tmp/test-100mb.txt

      - name: Create all compressed variants
        run: |
          mkdir -p /tmp/compressed
          
          # gzippy at all levels
          for level in 1 6 9; do
            ./bin/gzippy -$level -c /tmp/test-100mb.txt > /tmp/compressed/gzippy-L${level}.gz
          done
          
          # pigz at all levels  
          for level in 1 6 9; do
            ./bin/pigz -$level -c /tmp/test-100mb.txt > /tmp/compressed/pigz-L${level}.gz
          done
          
          # gzip at all levels
          for level in 1 6 9; do
            gzip -$level -c /tmp/test-100mb.txt > /tmp/compressed/gzip-L${level}.gz
          done
          
          ls -lh /tmp/compressed/

      - uses: actions/upload-artifact@v4
        with:
          name: test-data
          path: |
            /tmp/test-100mb.txt
            /tmp/compressed/
          retention-days: 1

  # ============================================================================
  # STAGE 3: Run comprehensive decompression benchmarks
  # ============================================================================
  benchmark:
    name: Decompress All Sources
    runs-on: ubuntu-latest
    needs: prepare-data
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: test-data
          path: ./data

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results

      - name: Run decompression benchmarks
        run: |
          ORIGINAL="./data/tmp/test-100mb.txt"
          ORIGINAL_SIZE=$(stat -c%s "$ORIGINAL")
          
          # Adaptive benchmarking parameters
          MIN_TRIALS=5           # Minimum iterations (avoid flukes)
          MAX_TRIALS=30          # Maximum iterations (prevent infinite loop)
          TARGET_CV=0.05         # Target coefficient of variation (stdev/mean < 5%)
          
          echo '{"benchmarks": [' > results/decompression.json
          first=true
          
          # Test each compressed file
          for gz_file in ./data/tmp/compressed/*.gz; do
            filename=$(basename "$gz_file" .gz)
            source=$(echo "$filename" | cut -d'-' -f1)
            level=$(echo "$filename" | cut -d'-' -f2 | tr -d 'L')
            
            echo "=== Testing $filename ==="
            
            # Test each decompressor at different thread counts
            for tool in gzippy pigz gzip rapidgzip; do
              for threads in 1 $(nproc); do
                # Skip invalid combinations
                if [ "$tool" = "gzip" ] && [ "$threads" != "1" ]; then
                  continue
                fi
                
                # Build command
                case "$tool" in
                  gzippy)   cmd="./bin/gzippy -d -p$threads" ;;
                  pigz)     cmd="./bin/unpigz -p$threads" ;;
                  gzip)     cmd="gzip -d" ;;
                  rapidgzip)
                    if [ -x "./bin/rapidgzip-cli" ]; then
                      cmd="./bin/rapidgzip-cli -d -P $threads"
                    else
                      continue  # Skip if not available
                    fi
                    ;;
                esac
                
                # Warmup run (discard - primes caches)
                $cmd < "$gz_file" > /tmp/out.dat 2>/dev/null
                
                # Verify correctness once
                if ! diff -q "$ORIGINAL" /tmp/out.dat > /dev/null 2>&1; then
                  echo "  $tool T$threads: FAILED (output mismatch)"
                  if [ "$first" = "true" ]; then first=false; else echo ',' >> results/decompression.json; fi
                  echo "{\"source\": \"$source\", \"level\": $level, \"tool\": \"$tool\", \"threads\": $threads, \"mean_time\": 0, \"stdev\": 0, \"speed_mbps\": 0, \"trials\": 0, \"status\": \"fail\"}" >> results/decompression.json
                  continue
                fi
                
                # Adaptive benchmarking: run until stable or max trials
                times=""
                trial=0
                converged=false
                
                while [ $trial -lt $MAX_TRIALS ]; do
                  trial=$((trial + 1))
                  
                  start=$(date +%s.%N)
                  $cmd < "$gz_file" > /tmp/out.dat 2>/dev/null
                  end=$(date +%s.%N)
                  
                  elapsed=$(echo "$end - $start" | bc)
                  times="$times $elapsed"
                  
                  # Check convergence after minimum trials
                  if [ $trial -ge $MIN_TRIALS ]; then
                    cv=$(echo "$times" | python3 -c "
          import sys
          import statistics
          times = [float(x) for x in sys.stdin.read().split()]
          mean = statistics.mean(times)
          stdev = statistics.stdev(times)
          cv = stdev / mean if mean > 0 else 1
          print(f'{cv:.4f}')
          ")
                    
                    # Check if coefficient of variation is below threshold
                    if python3 -c "exit(0 if $cv < $TARGET_CV else 1)"; then
                      converged=true
                      break
                    fi
                  fi
                done
                
                # Calculate final statistics
                stats=$(echo "$times" | python3 -c "
          import sys
          import statistics
          times = [float(x) for x in sys.stdin.read().split()]
          mean = statistics.mean(times)
          stdev = statistics.stdev(times) if len(times) > 1 else 0
          cv = stdev / mean if mean > 0 else 0
          speed = $ORIGINAL_SIZE / 1048576 / mean
          print(f'{mean:.4f} {stdev:.4f} {speed:.1f} {cv:.4f}')
          ")
                mean=$(echo "$stats" | cut -d' ' -f1)
                stdev=$(echo "$stats" | cut -d' ' -f2)
                speed=$(echo "$stats" | cut -d' ' -f3)
                cv=$(echo "$stats" | cut -d' ' -f4)
                
                conv_status=""
                if [ "$converged" = "true" ]; then
                  conv_status="converged"
                else
                  conv_status="max_trials"
                fi
                
                echo "  $tool T$threads: ${speed} MB/s (${trial} trials, CV=${cv}, ${conv_status})"
                
                # Output JSON
                if [ "$first" = "true" ]; then
                  first=false
                else
                  echo ',' >> results/decompression.json
                fi
                
                cat >> results/decompression.json << EOF
          {"source": "$source", "level": $level, "tool": "$tool", "threads": $threads, "mean_time": $mean, "stdev": $stdev, "speed_mbps": $speed, "trials": $trial, "cv": $cv, "converged": $converged, "status": "pass"}
          EOF
              done
            done
          done
          
          echo ']}' >> results/decompression.json
          cat results/decompression.json

      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 4: Generate summary and post to PR
  # ============================================================================
  summary:
    name: Decompression Summary
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()
    permissions:
      contents: read
      pull-requests: write
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: results/
        continue-on-error: true

      - name: Generate summary
        id: summary
        run: |
          if [ ! -f results/decompression.json ]; then
            echo "No benchmark results found"
            echo "## âš ï¸ Decompression Benchmarks" >> $GITHUB_STEP_SUMMARY
            echo "No results available" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi
          
          python3 << 'EOF' > summary.md
          import json
          import sys
          
          with open('results/decompression.json') as f:
              data = json.load(f)
          
          benchmarks = data.get('benchmarks', [])
          if not benchmarks:
              print("No benchmark data")
              sys.exit(0)
          
          print("## ðŸš€ Decompression Benchmark Results\n")
          
          # Group by source file
          by_source = {}
          for b in benchmarks:
              key = f"{b['source']}-L{b['level']}"
              if key not in by_source:
                  by_source[key] = []
              by_source[key].append(b)
          
          # Performance targets (Updated Jan 2026)
          # These encode our performance advantages:
          # - Fixed block turbo: 20% faster than libdeflate
          # - BGZF parallel: 2.9x libdeflate single-thread
          # - Multi-member parallel: 3000+ MB/s
          TARGETS = {
              'gzippy_single': 500,    # MB/s single-threaded (conservative, varies by data)
              'gzippy_multi': 2000,    # MB/s multi-threaded on BGZF/multi-member
              'vs_rapidgzip': 0.9,     # Ratio (>0.9 = acceptable, >1.0 = winning)
              'vs_libdeflate_fixed': 1.1,  # MUST beat libdeflate on fixed blocks
          }
          
          all_passed = True
          failures = []
          
          print("### Results by Source File\n")
          print("> Adaptive benchmarking: runs until CV < 5% or max 30 trials\n")
          
          for source_key in sorted(by_source.keys()):
              results = by_source[source_key]
              print(f"#### {source_key}\n")
              print("| Tool | Threads | Speed (MB/s) | CV | Trials | Status |")
              print("|------|---------|--------------|-----|--------|--------|")
              
              gzippy_speeds = {}
              rapidgzip_speeds = {}
              
              for r in sorted(results, key=lambda x: (x['tool'], x['threads'])):
                  status_icon = "âœ…" if r['status'] == 'pass' else "âŒ"
                  cv_pct = r.get('cv', 0) * 100
                  trials = r.get('trials', '?')
                  converged = r.get('converged', False)
                  conv_icon = "âœ“" if converged else f"({trials})"
                  print(f"| {r['tool']} | {r['threads']} | {r['speed_mbps']:.1f} | {cv_pct:.1f}% | {trials}{'' if converged else '*'} | {status_icon} |")
                  
                  if r['status'] != 'pass':
                      all_passed = False
                      failures.append(f"{source_key}: {r['tool']} T{r['threads']}")
                  
                  if r['tool'] == 'gzippy':
                      gzippy_speeds[r['threads']] = r['speed_mbps']
                  elif r['tool'] == 'rapidgzip':
                      rapidgzip_speeds[r['threads']] = r['speed_mbps']
              
              # Compare gzippy vs rapidgzip
              print()
              for threads in gzippy_speeds:
                  if threads in rapidgzip_speeds and rapidgzip_speeds[threads] > 0:
                      ratio = gzippy_speeds[threads] / rapidgzip_speeds[threads]
                      emoji = "ðŸ†" if ratio >= 1.0 else "ðŸ“‰"
                      print(f"{emoji} gzippy vs rapidgzip (T{threads}): **{ratio:.2f}x**")
              print()
          
          # Overall summary
          print("---\n")
          print("### Summary\n")
          if all_passed:
              print("âœ… **All decompression tests passed**")
          else:
              print("âŒ **Some tests failed:**")
              for f in failures:
                  print(f"- {f}")
          
          # Performance check
          gzippy_results = [b for b in benchmarks if b['tool'] == 'gzippy']
          if gzippy_results:
              max_threads = max(b['threads'] for b in gzippy_results)
              single_speed = max(b['speed_mbps'] for b in gzippy_results if b['threads'] == 1)
              multi_speed = max(b['speed_mbps'] for b in gzippy_results if b['threads'] == max_threads)
              
              print(f"\n**gzippy Performance:**")
              print(f"- Single-threaded: {single_speed:.0f} MB/s (target: {TARGETS['gzippy_single']} MB/s)")
              print(f"- Multi-threaded ({max_threads}T): {multi_speed:.0f} MB/s (target: {TARGETS['gzippy_multi']} MB/s)")
          EOF
          
          cat summary.md >> $GITHUB_STEP_SUMMARY
          cat summary.md

      - name: Post to PR
        if: github.event_name == 'pull_request'
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let body;
            try {
              body = fs.readFileSync('summary.md', 'utf8');
            } catch {
              body = '## Decompression Benchmarks\nNo results available';
            }
            
            body += '\n\n---\n*Updated: ' + new Date().toISOString() + '*';
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const marker = '## ðŸš€ Decompression Benchmark Results';
            const existing = comments.find(c => c.body && c.body.includes(marker));
            
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
