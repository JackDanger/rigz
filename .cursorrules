# gzippy - The Fastest Parallel Gzip

## Prime Directive

**gzippy must be the fastest gzip implementation possible within the constraints of:**
1. Full gzip/gunzip compatibility (RFC 1952)
2. Matching or beating pigz compression ratio at each level
3. Using the best available libraries (libdeflate, zlib-ng)

## Architecture

### Compression Levels

| Levels | Library | Strategy | Block Style |
|--------|---------|----------|-------------|
| L1-L5 | libdeflate | Parallel independent | BGZF markers |
| L6-L9 | zlib-ng | Pipelined with dictionary | Single stream |
| L10-L12 | libdeflate L10-12 | Parallel independent | BGZF markers, 512KB blocks |

### Decompression Architecture

| File Type | Strategy | Performance |
|-----------|----------|-------------|
| BGZF (gzippy output) | Parallel via embedded markers | ~1100 MB/s |
| Multi-member (pigz) | Parallel per-member | ~1100 MB/s |
| Single-member (gzip) | Optimized sequential | ~1050 MB/s |

### Key Decompression Modules

| File | Purpose |
|------|---------|
| `decompression.rs` | Main entry point, routes to appropriate backend |
| `ultra_decompress.rs` | Dispatcher for BGZF/multi-member/single |
| `rapidgzip_algo.rs` | Speculative parallel deflate (experimental) |
| `isal.rs` | ISA-L bindings with libdeflate fallback |
| `streaming_inflate.rs` | Indexing inflater with window tracking |

### L10-L12: Ultra Compression (Independent Blocks)
- **Library**: libdeflate L10-L12 (exhaustive search)
- **Strategy**: Large 512KB blocks for better context
- **Compression**: Near-zopfli ratio (4-5% smaller than gzip -9)
- **Decompression**: Parallel (blocks decompress independently)

## Performance Targets

| Level | Speed vs pigz | Size vs pigz |
|-------|---------------|--------------|
| L1-L5 | Must beat | Within 8% |
| L6-L9 | Must beat | Within 2% |
| L10-L12 | Must beat | Better ratio (ultra compression) |

## Decompression vs rapidgzip

rapidgzip achieves ~3000 MB/s through:
1. **Custom bit-level deflate decoder** - ~10K lines of C++
2. **ISA-L integration** - Hand-optimized SIMD
3. **Block-level parallelism** - Not just member-level
4. **Window caching** - 32KB window propagation

gzippy achieves ~1100 MB/s through:
1. **libdeflate** - Fast sequential decompression
2. **BGZF markers** - Perfect parallelism for gzippy-compressed files
3. **Multi-member detection** - Parallel for pigz output

**Key insight**: For one-time decompression of standard gzip files, parallelism 
requires a full deflate decoder that can find bit-aligned block boundaries. 
This is 3000+ lines of code not yet implemented.

## File Organization

| File | Purpose |
|------|---------|
| `parallel_compress.rs` | L1-L5 + L10-L12: libdeflate parallel blocks |
| `pipelined_compress.rs` | L6-L9: zlib-ng with dictionary sharing |
| `scheduler.rs` | Pigz-style N+1 thread scheduler |
| `decompression.rs` | Main decompression orchestrator |

## Threading Model

Adopted from pigz:
1. **N compress workers** claim blocks via atomic counter
2. **1 dedicated writer** writes blocks in order  
3. Workers never block on I/O
4. Brief spin-wait for low-latency handoff

## What Doesn't Work

1. **Independent blocks at L9**: 4% larger output (violates 0.5% threshold)
2. **Capping threads at physical_cores**: VMs underreport; use requested count
3. **Main thread helping compress**: Causes write delays
4. **Pure spin-wait**: Burns vCPU time on cloud VMs
5. **Speculative deflate block detection**: Deflate blocks are bit-aligned, 
   guessing byte offsets doesn't work reliably

## igzip Comparison

igzip (Intel ISA-L) is a **speed-optimized** compressor that trades compression ratio
for speed. It produces 10-15% larger files than gzip/pigz/gzippy at similar levels.

**Benchmark approach**: Compare gzippy against pigz (same compression goals), not igzip.
igzip is benchmarked for informational purposes but not as a pass/fail target.

## Build Optimizations

```toml
[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
panic = "abort"
strip = true
```

### ISA-L Integration

When available, ISA-L is linked via build.rs:
```
isa-l/build/libisal.dylib (macOS)
isa-l/build/libisal.so (Linux)
```

Enable with: `cargo build --release --features isal`

## Pre-commit Checks

A pre-commit hook runs:
1. `cargo fmt --check`
2. `cargo clippy --all-targets --all-features -- -D warnings`

To skip (emergency only): `git commit --no-verify`

## Debug Mode

Set `GZIPPY_DEBUG=1` to enable timing diagnostics during compression.

## Current Status (Jan 2026)

### Compression: ✅ Complete
- Beats pigz at all levels
- L10-L12 ultra compression (near-zopfli ratios)

### Decompression: ✅ Framework Complete
- ✅ Beats pigz and gzip in all scenarios
- ✅ BGZF parallel decompression (~1100 MB/s)
- ✅ Custom deflate decoder implemented (deflate_decoder.rs)
- ✅ Block boundary tracking and window caching
- ✅ Two-phase parallel inflate framework

### Gap to rapidgzip (~3000 MB/s)
rapidgzip achieves 3x speedup through:
1. **Speculative decode without window** - Decode first, resolve refs later
2. **Bit-aligned block finding** - Try all 8 bit offsets at each position
3. **Window propagation** - Forward windows through chunk chain
4. **Result stitching** - Merge parallel outputs with resolved refs

Current infrastructure is in place; full optimization requires:
- Tracking unresolved back-references during decode
- Multi-pass window propagation algorithm
- Lock-free result stitching
