# rigz - Absolute Maximum Performance Gzip

## Prime Directive

**rigz must be the fastest gzip implementation possible within the constraints of:**
1. Full gzip/gunzip compatibility (RFC 1952)
2. Matching or beating pigz compression ratio at each level and thread count
3. Full use of forking libraries or fully reimplementing with hyperoptimizations

**Time is free. Labor is free. We can do infinite work. The goal is theoretical maximum performance.**

## Performance Hierarchy

### L1-L6: Speed-Optimized (Independent Blocks)
- **Library**: libdeflate (30-50% faster than zlib-ng)
- **Strategy**: Parallel independent blocks with BGZF markers
- **Decompression**: Parallel (each block decompresses independently)
- **Target**: 2-3x faster than pigz

### L7-L9: Compression-Optimized (Pipelined)
- **Library**: Best available (zlib-ng, ISA-L, or custom)
- **Strategy**: Dictionary sharing between blocks (like pigz)
- **Decompression**: Sequential (single gzip stream)
- **Target**: Match pigz compression, beat pigz speed

## L9 Optimization Strategy (Current Focus)

### Compression Targets
- **Speed**: Must beat pigz by at least 10%
- **Size**: Must match pigz within 0.5%
- **Single-thread**: Beat pigz by 40%+
- **Multi-thread**: Beat pigz by 20%+

### Decompression Targets
- **Speed**: At least match pigz (sequential output)
- **Use libdeflate**: Already fastest for single-stream

## Optimization Techniques (Exhaustive)

### Tier 1: Already Implemented
- [x] libdeflate for L1-L6 compression
- [x] libdeflate for all decompression
- [x] Parallel block compression with rayon
- [x] BGZF markers for parallel decompression (L1-L6)
- [x] Pipelined compression for L7-L9
- [x] Thread-local buffer reuse
- [x] Memory-mapped I/O
- [x] Dynamic block sizing based on file size
- [x] Parallel CRC32 calculation
- [x] Hardware CRC32 detection

### Tier 2: Implemented
- [x] Thread-local Compress objects (zlib-ng for L7-L9)
- [x] Thread-local libdeflate Compressor (L1-L6)
- [x] Pre-allocated output buffers (thread-local Vec reuse)
- [x] Thread-local Decompressor and buffers
- [x] Dynamic block sizing by file size (larger for 4-core systems)
- [ ] Memory prefetching (madvise MADV_SEQUENTIAL)
- [ ] Write combining for output

### Tier 3: Aggressive Optimizations
- [ ] Intel ISA-L for L9 compression (if available)
- [ ] AVX-512 optimized match finding
- [ ] Huge pages for large allocations (>2MB)
- [ ] Core pinning (NUMA-aware thread placement)
- [ ] Lock-free work stealing for better load balance
- [ ] Batch syscalls (io_uring on Linux)

### Tier 4: Extreme Optimizations
- [ ] Custom allocator (arena per thread)
- [ ] Profile-guided optimization (PGO build)
- [ ] Link-time optimization (LTO)
- [ ] CPU-specific builds (x86-64-v3, x86-64-v4)
- [ ] Inline assembly for hot paths
- [ ] Branch prediction hints (__builtin_expect)

## Architecture Decisions

### Why libdeflate for L1-L6?
- 30-50% faster than zlib-ng
- Better SIMD utilization
- No dictionary needed for independent blocks
- No zlib-ng L1 RLE issue

### Why zlib-ng for L7-L9?
- Supports deflateSetDictionary (libdeflate doesn't)
- Dictionary sharing is essential for L9 compression ratio
- Still faster than system zlib

### Why pipelined for L7-L9?
- Dictionary sharing requires sequential block dependency
- BUT: Block N only needs block N-1's INPUT, not OUTPUT
- So blocks can be compressed in parallel (pigz's insight)

## What Doesn't Work

1. **Independent blocks at L9**: 4% larger output (violates 0.5% threshold)
2. **Hybrid chain compression**: Gzip format can't embed dictionaries
3. **Forward-refs-only**: Hash priming provides 0% compression benefit
4. **Overlapping blocks**: Standard tools output redundant data
5. **libdeflate L12**: 4x slower than L9 for marginal gain
6. **Small block sizes on 4-core systems**: More coordination overhead than benefit

## Architecture-Dependent Performance

### Apple Silicon (M4 Pro, 14 cores)
- L9 T4 compression: **35-40% faster** than pigz
- L9 decompression: **20-50% faster** than pigz

### Intel x86_64 (i7-13700T, 16 cores)  
- L9 T4-T16 compression: **27-35% faster** than pigz
- L9 decompression: **50-100% faster** than pigz

### GHA Runners (AMD EPYC 7763 / Intel Xeon Platinum 8370C, 4 vCPU)
- L9 T4 compression: **35-40% faster** than pigz (after optimization)
- L9 decompression: **48% faster** than pigz
- **Key fix**: Minimize block count to ~8 blocks (2 per thread) to reduce coordination overhead

### Key Insights
1. Too many small blocks = too much synchronization overhead on 4-core systems.
2. Block size = file_size / (threads * 2), clamped to 256KB-4MB.
3. On real hardware (Apple Silicon, Intel x86), rigz is 30-40% faster at L9 compression.
4. The compression+decompression total time strongly favors rigz.

## What Doesn't Work

1. **Independent blocks at L9**: 4% larger output (violates 0.5% threshold)
2. **Hybrid chain compression**: Gzip format can't embed dictionaries
3. **Forward-refs-only**: Hash priming provides 0% compression benefit
4. **Overlapping blocks**: Standard tools output redundant data
5. **libdeflate L12**: 4x slower than L9 for marginal gain
6. **Small block sizes on 4-core systems**: More coordination overhead than benefit

## Performance Testing

### Required Tests
- All levels (1, 3, 6, 7, 9)
- Single-thread and max-threads
- File sizes: 1MB, 10MB, 100MB
- Data types: text, random, binary/tarball

### Thresholds
- L1-L6: Speed must beat pigz, size within 8%
- L7-L8: Speed must beat pigz, size within 2%
- L9: Speed must beat pigz, size within 0.5%

### Statistical Significance
- Minimum 5 runs per test
- Use median (not mean)
- Welch's t-test for significance
- 2% tolerance for CI noise

## Build Optimizations

```toml
[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
panic = "abort"
strip = true
```

## File Organization

| File | Purpose |
|------|---------|
| `parallel_compress.rs` | L1-L6: libdeflate parallel blocks |
| `pipelined_compress.rs` | L7-L9: zlib-ng with dictionary sharing |
| `decompression.rs` | libdeflate for all paths |
| `libdeflate_ext.rs` | Direct libdeflate-sys bindings |
| `optimization.rs` | CPU detection, buffer sizing |

## The Path to Absolute Maximum

1. **Tier 2 optimizations** will get us 10-20% faster
2. **Tier 3 optimizations** will get us another 10-15%
3. **Tier 4 optimizations** will squeeze out the last 5-10%
4. **Total expected gain**: 25-45% faster than current

After all optimizations, rigz should be:
- **L9 single-thread**: 50-60% faster than pigz
- **L9 multi-thread**: 30-40% faster than pigz
- **L9 decompression**: Equal to pigz (both use sequential libdeflate)

This represents the theoretical maximum without:
- Custom DEFLATE implementation
- Hardware acceleration beyond CPU SIMD
- New compression algorithms

## Notes From Recent Work

- L9 pipelined path uses `PipelinedGzEncoder` with a fixed 128KB block size and
  32KB dictionaries (last input bytes), writing a single gzip stream.
- Parallel pipeline uses a custom scheduler with spin-waited ordered writes;
  per-block CRCs are computed in workers then combined in-order after all blocks.

## GHA-Specific Optimizations (4 vCPU)

### What Works
1. **N workers compress, main thread writes**: Spawn N worker threads that all
   compress; main thread only writes in order. Maximizes parallelism.
2. **Hybrid spin/condvar wait**: Brief 64-spin, then condvar with 10Î¼s timeout.
   Balances latency and CPU efficiency.
3. **Condvar-based signaling**: Workers signal completion via condvar, main
   thread sleeps efficiently instead of burning CPU.
4. **192KB blocks for files >= 50MB**: Reduces coordination overhead from 780
   to 520 blocks, while staying within compression ratio limits.
5. **madvise MADV_SEQUENTIAL**: Hint for mmap prefetching.
6. **Conservative slot capacity**: block_size + 10% + 1KB to handle worst case.

### What Doesn't Work on GHA
1. **Main thread helping compress**: Causes write delays and serialization.
2. **256KB blocks for all files**: Exceeds 0.5% size threshold on random data.
3. **Pure spin-wait**: Burns vCPU time, reduces compression throughput.
4. **Long condvar timeouts**: Adds latency between block writes.

### GHA vs Local Performance
| Metric | Local (Apple M4) | GHA (Intel Xeon 4vCPU) |
|--------|------------------|------------------------|
| 10MB L9 | 25-30% faster | ~5% faster (threshold) |
| 100MB L9 | 35-40% faster | ~5% faster (threshold) |

The difference is due to: virtualization overhead, shared runners, and CPU cache
characteristics. Optimizations must target GHA's constrained environment.
