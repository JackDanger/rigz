# rigz - Absolute Maximum Performance Gzip

## Prime Directive

**rigz must be the fastest gzip implementation possible within the constraints of:**
1. Full gzip/gunzip compatibility (RFC 1952)
2. Matching or beating pigz compression ratio at each level
3. No new mathematical breakthroughs required

**Time is free. We can do infinite work. The goal is theoretical maximum performance.**

## Performance Hierarchy

### L1-L6: Speed-Optimized (Independent Blocks)
- **Library**: libdeflate (30-50% faster than zlib-ng)
- **Strategy**: Parallel independent blocks with BGZF markers
- **Decompression**: Parallel (each block decompresses independently)
- **Target**: 2-3x faster than pigz

### L7-L9: Compression-Optimized (Pipelined)
- **Library**: Best available (zlib-ng, ISA-L, or custom)
- **Strategy**: Dictionary sharing between blocks (like pigz)
- **Decompression**: Sequential (single gzip stream)
- **Target**: Match pigz compression, beat pigz speed

## L9 Optimization Strategy (Current Focus)

### Compression Targets
- **Speed**: Must beat pigz by at least 10%
- **Size**: Must match pigz within 0.5%
- **Single-thread**: Beat pigz by 40%+
- **Multi-thread**: Beat pigz by 20%+

### Decompression Targets
- **Speed**: At least match pigz (sequential output)
- **Use libdeflate**: Already fastest for single-stream

## Optimization Techniques (Exhaustive)

### Tier 1: Already Implemented
- [x] libdeflate for L1-L6 compression
- [x] libdeflate for all decompression
- [x] Parallel block compression with rayon
- [x] BGZF markers for parallel decompression (L1-L6)
- [x] Pipelined compression for L7-L9
- [x] Thread-local buffer reuse
- [x] Memory-mapped I/O
- [x] Dynamic block sizing based on file size
- [x] Parallel CRC32 calculation
- [x] Hardware CRC32 detection

### Tier 2: Implemented
- [x] Thread-local Compress objects (zlib-ng for L7-L9)
- [x] Thread-local libdeflate Compressor (L1-L6)
- [x] Pre-allocated output buffers (thread-local Vec reuse)
- [x] Thread-local Decompressor and buffers
- [x] Dynamic block sizing by file size (larger for 4-core systems)
- [ ] Memory prefetching (madvise MADV_SEQUENTIAL)
- [ ] Write combining for output

### Tier 3: Aggressive Optimizations
- [ ] Intel ISA-L for L9 compression (if available)
- [ ] AVX-512 optimized match finding
- [ ] Huge pages for large allocations (>2MB)
- [ ] Core pinning (NUMA-aware thread placement)
- [ ] Lock-free work stealing for better load balance
- [ ] Batch syscalls (io_uring on Linux)

### Tier 4: Extreme Optimizations
- [ ] Custom allocator (arena per thread)
- [ ] Profile-guided optimization (PGO build)
- [ ] Link-time optimization (LTO)
- [ ] CPU-specific builds (x86-64-v3, x86-64-v4)
- [ ] Inline assembly for hot paths
- [ ] Branch prediction hints (__builtin_expect)

## Architecture Decisions

### Why libdeflate for L1-L6?
- 30-50% faster than zlib-ng
- Better SIMD utilization
- No dictionary needed for independent blocks
- No zlib-ng L1 RLE issue

### Why zlib-ng for L7-L9?
- Supports deflateSetDictionary (libdeflate doesn't)
- Dictionary sharing is essential for L9 compression ratio
- Still faster than system zlib

### Why pipelined for L7-L9?
- Dictionary sharing requires sequential block dependency
- BUT: Block N only needs block N-1's INPUT, not OUTPUT
- So blocks can be compressed in parallel (pigz's insight)

## What Doesn't Work

1. **Independent blocks at L9**: 4% larger output (violates 0.5% threshold)
2. **Hybrid chain compression**: Gzip format can't embed dictionaries
3. **Forward-refs-only**: Hash priming provides 0% compression benefit
4. **Overlapping blocks**: Standard tools output redundant data
5. **libdeflate L12**: 4x slower than L9 for marginal gain
6. **Small block sizes on 4-core systems**: More coordination overhead than benefit

## Architecture-Dependent Performance

### Apple Silicon (M4 Pro, 14 cores)
- L9 T4 compression: **35-40% faster** than pigz
- L9 decompression: **20-50% faster** than pigz

### Intel x86_64 (i7-13700T, 16 cores)  
- L9 T4-T16 compression: **27-35% faster** than pigz
- L9 decompression: **50-100% faster** than pigz

### GHA Runners (AMD EPYC 7763, 4 vCPU)
- L9 T4 compression: **5-7% slower** than pigz (threshold: 5%)
- L9 decompression: **48% faster** than pigz
- **Mitigation**: Larger block sizes (512KB for 100MB files) reduce coordination overhead

### Key Insight
Block coordination overhead is relatively higher on 4-core VMs. Larger block sizes help.

## Performance Testing

### Required Tests
- All levels (1, 3, 6, 7, 9)
- Single-thread and max-threads
- File sizes: 1MB, 10MB, 100MB
- Data types: text, random, binary/tarball

### Thresholds
- L1-L6: Speed must beat pigz, size within 8%
- L7-L8: Speed must beat pigz, size within 2%
- L9: Speed must beat pigz, size within 0.5%

### Statistical Significance
- Minimum 5 runs per test
- Use median (not mean)
- Welch's t-test for significance
- 2% tolerance for CI noise

## Build Optimizations

```toml
[profile.release]
opt-level = 3
lto = "fat"
codegen-units = 1
panic = "abort"
strip = true
```

## File Organization

| File | Purpose |
|------|---------|
| `parallel_compress.rs` | L1-L6: libdeflate parallel blocks |
| `pipelined_compress.rs` | L7-L9: zlib-ng with dictionary sharing |
| `decompression.rs` | libdeflate for all paths |
| `libdeflate_ext.rs` | Direct libdeflate-sys bindings |
| `optimization.rs` | CPU detection, buffer sizing |

## The Path to Absolute Maximum

1. **Tier 2 optimizations** will get us 10-20% faster
2. **Tier 3 optimizations** will get us another 10-15%
3. **Tier 4 optimizations** will squeeze out the last 5-10%
4. **Total expected gain**: 25-45% faster than current

After all optimizations, rigz should be:
- **L9 single-thread**: 50-60% faster than pigz
- **L9 multi-thread**: 30-40% faster than pigz
- **L9 decompression**: Equal to pigz (both use sequential libdeflate)

This represents the theoretical maximum without:
- Custom DEFLATE implementation
- Hardware acceleration beyond CPU SIMD
- New compression algorithms
