# rigz - Rust Parallel Gzip

Fast, parallel gzip replacement. Beats gzip and pigz by 40-50%.

## Quick Start

```bash
make           # Build + quick benchmark
make validate  # Verify output works with gunzip
cargo test     # Unit tests (requires 'all' permissions for debug build)
```

## Core Algorithm (parallel_compress.rs)

```
Input â†’ mmap â†’ [chunk1, chunk2, ...] â†’ rayon parallel â†’ [gzip1, gzip2, ...] â†’ concatenate â†’ Output
```

Each 128KB chunk becomes a complete gzip member. RFC 1952 allows concatenation.

## Key Files

| File | What it does |
|------|--------------|
| `parallel_compress.rs` | **THE CORE**: rayon + flate2 parallel compression |
| `compression.rs` | Routes single vs multi-threaded |
| `decompression.rs` | libdeflate (fast) for all paths |
| `libdeflate_ext.rs` | DecompressorEx wrapper for `_ex` functions |
| `optimization.rs` | CPU detection, buffer sizing, content analysis |
| `cli.rs` | gzip-compatible argument parsing |

## Performance Strategy

**Compression:**
- zlib-ng via flate2 (2-3x faster than standard zlib)
- **Level 1 â†’ Level 2 mapping** (zlib-ng L1 produces 2-5x larger output)
- 128KB fixed blocks (dynamic sizing regressed 14%)
- Global rayon pool (avoids per-call initialization)
- Thread-local buffer reuse (reduces allocator pressure)

**Decompression:**
- Single-member: libdeflate (30-50% faster than zlib)
- Multi-member: libdeflate via DecompressorEx (returns consumed bytes)
- SIMD header detection via memchr (10-50x faster scanning)
- ISIZE trailer hint for accurate buffer pre-allocation
- Thread-local decompressor reuse (avoids init overhead)
- Cache-aligned buffer allocation (better memory access)
- `libdeflate_ext.rs`: Uses libdeflate-sys directly for `_ex` functions

**Architecture Awareness:**
- Runtime CPU feature detection (AVX2, AVX-512, NEON, CRC32)
- L2 cache size detection for optimal block sizing
- Platform-specific defaults (Apple Silicon 128-byte cache lines)
- Physical core count for thread limiting

## Implemented Optimizations (P0/P1)

1. âœ… **Thread-local compression buffers** - Reuse buffers across parallel blocks
2. âœ… **SIMD multi-member detection** - memchr for fast gzip header scanning
3. âœ… **Cache-aligned buffers** - 64-byte x86, 128-byte Apple Silicon
4. âœ… **ISIZE buffer sizing** - Read gzip trailer for accurate pre-allocation
5. âœ… **Thread-local decompressor** - Reuse libdeflate decompressor
6. âœ… **CPU feature detection** - Runtime AVX/NEON/CRC32 detection
7. âœ… **Vectorized I/O** - write_vectored for reduced syscalls in compression
8. âœ… **Statistical validation** - 5+ trials per test with median/stdev reporting
9. âœ… **BGZF-style block markers** - Embed block sizes in FEXTRA for instant boundary detection
10. âœ… **Parallel BGZF decompression** - rigzâ†’rigz uses parallel libdeflate via rayon
11. âœ… **libdeflate _ex bindings** - Direct libdeflate-sys access for gzip_decompress_ex

## Future Optimizations (see OPTIMIZATION_OPPORTUNITIES.md)

- ðŸ”² io_uring async I/O on Linux (medium complexity)

## What Doesn't Work

1. **Manual deflate boundary detection** - Deflate streams contain false gzip headers
2. **Dynamic block sizing** - Added complexity, 14% slower
3. **gzp crate** - Threading issues; custom rayon is better
4. **Parallel libdeflate per-member (without markers)** - Boundary detection requires full inflate (2x overhead)
5. **zlib-ng Level 1 directly** - Uses RLE strategy, produces 2-5x larger files on repetitive data (we map L1â†’L2)
6. **Parallel decompression without BGZF** - Requires finding member boundaries first, which means decompressing once to find them, then again in parallel. 2x overhead made it slower for files with many small members (like rigz's 128KB chunks). Sequential MultiGzDecoder is faster.
7. **Intel ISA-L integration** - Requires autotools/autoreconf to build from source, complex build setup. The isal-rs crate fails on systems without autotools. Not worth the portability issues for marginal gains since libdeflate already uses SIMD.

## Architecture-Dependent Performance

Performance varies significantly by CPU architecture:

**Apple Silicon (M1/M2/M3/M4):**
- rigz beats pigz everywhere (10-43% faster)
- NEON optimizations in zlib-ng work well
- Larger L2 cache benefits block-based approach

**x86_64 Linux (GHA runners, 2 cores):**
- L1: rigz wins (17% faster)
- L6 max-threads: pigz wins (12% faster)
- L9 max-threads: pigz wins (32% faster)
- Decompression: rigz always wins (24% faster)

**Root cause:** At high compression levels with max threads, rigz's independent-block approach (restarts dictionary each block) has more overhead than pigz's pipelined shared-dictionary approach on x86. This is a fundamental trade-off: independent blocks enable parallel decompression but cost some compression efficiency.

**Accepted trade-off:** rigz prioritizes:
1. Parallel decompression (24% faster)
2. Apple Silicon performance (dominant in developer laptops)
3. Single-threaded performance (35-50% faster)
4. L1 parallel performance (17% faster)

## Test Suite

**Data Types** (realistic workloads, not just random):
- **text**: Proust (Project Gutenberg) - highly compressible, seed checked into git
- **random**: /dev/urandom - poorly compressible, stress test
- **tarball**: repo archive - mixed content, real-world simulation

**Key Commands:**
- `make validate` - Quick validation (local)
- `make validate-json` - Output JSON for visualization
- `make validation-chart` - Generate HTML performance chart
- `make test-data` - Generate 10MB + 100MB test files

**CI Workflow:**
- Tests all 3 data types Ã— 3 levels Ã— 2 thread counts = 54 compression configs
- Cross-tool decompression matrix (gzipâ†”pigzâ†”rigz) = 162 tests per run
- Generates PR comments with speedup summary
- Uploads validation-results.json artifact

**Benchmarking:**
- 5+ trials per test for statistical significance
- Use median, not min
- pigz built from submodule (./pigz/pigz)

See CONTRIBUTING.md for detailed architecture guide.
